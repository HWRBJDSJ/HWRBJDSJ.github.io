# HDFS 
- 设计思想 分而治之 :将大文件,大批量文件,分布式存放在大量服务器上,以便于采取分而治之的方式对海量数据进行运算分析
- 在大数据系统中的作用: 为各类分布式运算框架(mapreduce,spark,tez,...)提供数据存储
- 重点概念 : 文件切块,副本存放,元数据

## HDFS 是一个文件系统 是分布式的
- 重要特征如下:
	- 1)HDFS中的文件在物理上!分块存储(block)! 块的大小可以通过配置参数来规定,默认大小在hadoop2.x中是128M的 老版本是64M
	- 2)HDFS文件系统会给客户端提供一个!统一的抽象目录树!,客户端通过路径访问文件
	- 3)目录结构及文件分块信息(元数据)的管理由namenode节点承担({namenode是HDFS集群主节点,负责维护整个hdfs文件系统的目录树,以及每一个路径(文件)所对应的block块信息(block的ID,及所在的datanode服务器)})
	- 4) 文件的各个block的存储管理由datanode节点承担
	- 5) HDFS是设计成适应一次写入,多次读出的场景,且不支持文件的修改
##  HDFS的shell(命令行客户端)操作

### HDFS提供shell命令行客户端  命令主题为 :  hadoop fs -命令

### 命令行客户端支持的命令参数
- ?????????????

### 常用命令参数介绍  (在这些参数中,所有的hdfs路径都可以简写为 / )
- -help :输出这个命令参数手册 
- -ls   :显示目录信息   
- -mkdir : 在HDFS上创建目录
- -moveFromLocal : 从本地剪切粘贴到HDFS
- -moveToLocal  : 冲HDFS剪切粘贴到本地
- -appendToFile : 追加一个文件到已经存在的文件末尾
- -cat : 显示文件内容
- -tail : 显示一个文件的末尾
- -text : 以字符形式打印一个文件的内容
- -chgrp -chomd -chown  : 与Linux文件系统中的用法一样,对文件的所属权限进行修改
- -copyFromLocal : 从本地文件系统中拷贝文件到HDFS中
- -copyToLocal : 从HDFS拷贝到本地
- -cp   : 从HDFS的一个路径上拷贝到HDFS的另一个路径上
- -mv   : 从HDFS目录上移动文件
- -get  : 等同于copy
- -getmerge : 合并下载多个文件(需要是相同类型的文件)
- -put  : 等同于 copyFromLocal
- -rm   : 删除文件或者文件夹  (删除文件夹时加上  -r  递归)
- -rmdir : 删除空目录
- -df   : 统计文件系统的可用空间信息
- -du   : 统计文件夹的大小信息
- -count : 统计一个指定目录下饿得文件节点数量
- -setrep : 设置HDFS中文件的副本数量

# HDFS的工作机制

## HDFS的概述
- HDFS集群氛围两大角色 :Namenode Datanode (Secondary Namenode)
- Namenode负责管理整个文件系统的元数据
- Datanode负责管理用户的文件数据块
- 文件会按照固定的大小(blocksize)切成若干块后分布式存储在若干Datanode上 
- 每个文件块都有多个副本,并存放在不同的Datanode上
- Datanode会定期向Namenode汇报自身所保存的文件block 信息,而Namenode会负责保持文件的副本数量
- HDFS的内部工作机制对客户端保持透明,客户端请求访问HDFS都是通过Namenode申请来进行

## HDFS写数据流程
- 客户端要向HDFS写数据，首先要跟namenode通信以确认可以写文件并获得接收文件block的datanode，
- 然后，客户端按顺序将文件逐个block传递给相应datanode，并由接收到block的datanode负责向其他
- datanode复制block的副本

![HDFS写入流程图.png](https://upload-images.jianshu.io/upload_images/14467497-1954e0f1ed54f880.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
- 1、根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在
- 2、namenode返回是否可以上传
- 3、client请求第一个 block该传输到哪些datanode服务器上
- 4、namenode返回3个datanode服务器ABC
- 5、client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会
-  继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端
- 6、client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，
- A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答
- 7、当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。

## HDFS读数据流程
- 客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）
- 返回给客户端，客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追
- 加合并从而获得整个文件
![HDFS读取流程图.png](https://upload-images.jianshu.io/upload_images/14467497-5f9c3602da96e3a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 1、跟namenode通信查询元数据，找到文件块所在的datanode服务器
- 2、挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流
- 3、datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）
- 4、客户端以packet为单位接收，现在本地缓存，然后写入目标文件

# Namenode的工作机制
- 职责  : 负责客户端请求的响应, 元数据的管理(查询,修改)

## 元数据的管理
- Namenode 对数据的管理采用三种存储形式:
	- 内存元数据(NameSystem) (内存存储数据的弊端:在关闭后内存数据清空)
	- 磁盘元数据镜像文件
	- 数据操作日志文件(可通过日志结合镜像文件运算出元数据)

### 元数据的存储机制
- A、内存中有一份完整的元数据(内存meta data)
- B、磁盘有一个“准完整”的元数据镜像（fsimage）文件(在namenode的工作目录中)
- C、用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（edits文件）

### 元数据手动查看
- 可以通过HDFS的一个工具查看edits中的方法
- bin/hdfs oev -i edits -o edits.xml
- bin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml 
--???????????????????????????

### 元数据的checkpoint
- 每隔一段时间,会由secondary namenode将namenode上积累的所有edits 和一个最新的fsimage 
- 下载到本地,并加载到内存进行merge( 这个过程称为checkpoint)

#### checkpoint的详细过程
![checkpoint流程图.png](https://upload-images.jianshu.io/upload_images/14467497-b52eeabbddd2d3c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


- checkpoint 操作的触发条件配置参数
- dfs.namenode.checkpoint.check.period=60  #检查触发条件是否满足的频率，60秒
- dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary
- #以上两个参数做checkpoint操作时，secondary namenode的本地工作目录
- dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}

- dfs.namenode.checkpoint.max-retries=3  #最大重试次数
- dfs.namenode.checkpoint.period=3600  #两次checkpoint之间的时间间隔3600秒
- dfs.namenode.checkpoint.txns=1000000 #两次checkpoint之间最大的操作记录

#### checkpoint的附带作用
- namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，
- 可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据

### 元数据目录说明

在第一次部署好Hadoop集群的时候，我们需要在NameNode（NN）节点上格式化磁盘 ：
$HADOOP_HOME/bin/hdfs namenode -format

格式化完成之后，将会在$dfs.namenode.name.dir/current目录下如下的文件结构 : 
current/
|-- VERSION
|-- edits_*
|-- fsimage_0000000000008547077
|-- fsimage_0000000000008547077.md5
`-- seen_txid

其中的dfs.name.dir是在hdfs-site.xml文件中配置的，默认值如下：
<property>
  <name>dfs.name.dir</name>
  <value>file://${hadoop.tmp.dir}/dfs/name</value>
</property>

hadoop.tmp.dir是在core-site.xml中配置的，默认值如下
<property>
  <name>hadoop.tmp.dir</name>
  <value>/tmp/hadoop-${user.name}</value>
  <description>A base for other temporary directories.</description>
</property>

dfs.namenode.name.dir属性可以配置多个目录，如/data1/dfs/name,/data2/dfs/name,/data3/dfs/name,....。
各个目录存储的文件结构和内容都完全一样，相当于备份，这样做的好处是当其中一个目录损坏了，也不会影响
到Hadoop的元数据，特别是当其中一个目录是NFS（网络文件系统Network File System，NFS）之上，
即使你这台机器损坏了，元数据也得到保存。


下面对$dfs.namenode.name.dir/current/目录下的文件进行解释。

1、VERSION文件是Java属性文件，内容大致如下：

#Fri Nov 15 19:47:46 CST 2013
namespaceID=934548976
clusterID=CID-cdff7d73-93cd-4783-9399-0a22e6dce196
cTime=0
storageType=NAME_NODE
blockpoolID=BP-893790215-192.168.24.72-1383809616115
layoutVersion=-47


其中
　　（1）namespaceID是文件系统的唯一标识符，在文件系统首次格式化之后生成的；
　　（2）storageType说明这个文件存储的是什么进程的数据结构信息（如果是DataNode，storageType=DATA_NODE）；
　　（3）cTime表示NameNode存储时间的创建时间，由于我的NameNode没有更新过，所以这里的记录值为0，以后对NameN
		 ode升级之后，cTime将会记录更新时间戳；
　　（4）layoutVersion表示HDFS永久性数据结构的版本信息， 只要数据结构变更，版本号也要递减，此时的HDFS也需要升级，
		 否则磁盘仍旧是使用旧版本的数据结构，这会导致新版本的NameNode无法使用；
　　（5）clusterID是系统生成或手动指定的集群ID，在-clusterid选项中可以使用它；如下说明
			a、使用如下命令格式化一个Namenode：
			$HADOOP_HOME/bin/hdfs namenode -format [-clusterId <cluster_id>]
			选择一个唯一的cluster_id，并且这个cluster_id不能与环境中其他集群有冲突。
			如果没有提供cluster_id，则会自动生成一个唯一的ClusterID。
			b、使用如下命令格式化其他Namenode：
			$HADOOP_HOME/bin/hdfs namenode -format -clusterId <cluster_id>
			c、升级集群至最新版本。在升级过程中需要提供一个ClusterID，例如：
			$HADOOP_PREFIX_HOME/bin/hdfs start namenode --config $HADOOP_CONF_DIR? -upgrade -clusterId <cluster_ID>
			如果没有提供ClusterID，则会自动生成一个ClusterID。
　　（6）blockpoolID：是针对每一个Namespace所对应的blockpool的ID，上面的这个BP-893790215-192.168.24.72-1383809616115就是在
			我的ns1的namespace下的存储块池的ID，这个ID包括了其对应的NameNode节点的ip地址。
　　
2、$dfs.namenode.name.dir/current/seen_txid非常重要，是存放transactionId的文件，format之后是0，它代表的是namenode里面的edits_*文件的尾数，
namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字。所以当你的hdfs发生异常重启的时候，一定要比对seen_txid内的
数字是不是你edits最后的尾数，不然会发生建置namenode时metaData的资料有缺少，导致误删Datanode上多余Block的资讯。

3、$dfs.namenode.name.dir/current目录下在format的同时也会生成fsimage和edits文件，及其对应的md5校验文件。


补充：seen_txid 
文件中记录的是edits滚动的序号，每次重启namenode时，namenode就知道要将哪些edits进行加载edits

# Datanode的工作机制

## datanode的工作职责 : 存储管理用户的文件块数据 定期向namenode汇报自身所持有的block信息(通过心跳信息上报)
- 心跳机制 : 每过一段时间就向namenode汇报自身的数据 当集群发生某些block副本失效时,集群及时回复block初识副本数量问题
![心跳时间.png](https://upload-images.jianshu.io/upload_images/14467497-22264c01e7f73e1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## Datanode掉线判断时限参数
- datanode进程死亡或者网络故障造成datanode无法与namenode通信,namenode不会立即判断该节点死亡,要经过一段时间,这个时间
- 暂称为超时时长,HDFS的默认超时时长为10分钟+30秒. 如果定义超长时间为timeout,则超时时长的计算公式是:
- timeout  = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval。
- 默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。
	- 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，
	- dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval
	- 设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。

![heartbeat.png](https://upload-images.jianshu.io/upload_images/14467497-f7b77689eb294ab9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 观察验证Datanode功能
- 上传一个文件,观察文件的block具体的物理存放情况:
- 在每一台datanode机器上的这个目录找到文件的切块:
- /home/hadoop/app/hadoop-2.4.1/tmp/dfs/data/current/BP-193442119-192.168.2.120-1432457733977/current/finalized



