# MapReduce原理

combiner 的使用时机
MapReduce的过程中  map的数据在每个块传到reduce之前会有combiner合并的过程
combiner合并过程可以看成为一个小reduce过程 与reduce过程一致 也可以自己编辑




## 自定义combiner 要继承Reducer
   重写reduce 方法
   
   
map到reduce阶段被称为shuffle阶段  排序 分组 combiner 都是在这个阶段发生的


MapReduce的细致过程：
MRappMaster    这个MapReduce任务的管理者  负责调度及状态协调
Maptask  	   map任务 	负责map阶段的整个数据处理流程
reduceTask	   reduce任务	分组reduce阶段的整个数据处理流程


运行一个MapReduce  需要开启多少maptask  多少reducetask   
目测是有block决定的   实际是有getsplit/切片决定的 getsplit=blocksize的
实际与block无关  一个block可以切成一个片 也可用切多片
有多少个maptask是由切片有多少决定的
注意  ~！！！！  当要读取多个文件 是 哪怕有一个文件很小 也不会与其他文件共用切片 也会是一个切片  
假设 一个a.txt 存放在三个datanode中   每个datanode中会根据切片数量生成多个maptask
maptask会调用map方法


切片如何切？
文件通过fileInputformat 加到job中  在这个方法中有一个getsplit方法
切片是在job提交时候才会进行
jobsubmit原理 ？？？？？？？？？？
切块的大小：  Math.max(minsize,math.min(maxSize,blocksize));
切块的数量：  每达到一个切块的大小就会切一个块  ，当剩余的大小不超过一个块的10% 会与前面的块一起合并在一起



job.write 将信息交给yarn或local  会给其返回一个路径 将信息保存
yarn返回的路径在HDFS上  local返回的就在本地
生成三个文件 job.xml  job.split job.jar 
传到yarn上后   MPAppmaster就出现了（在datanode上）  
将那三个文件获取后
生成maptask 
再向yarn申请资源  实际是向resourceManager申请资源
其他每个datanode上都有一个nodemanager  resourceManager和nodemanager之间有心跳



1、	一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的mapta
	sk实例数量，然后向集群申请机器启动相应数量的maptask进程

2、	maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为：
	a)利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对
	b)将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存
	c)将缓存中的KV对按照K分区排序后不断溢写到磁盘文件

3、	MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知
	reducetask进程要处理的数据范围（数据分区）

4、	Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个
	maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行
	逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储


