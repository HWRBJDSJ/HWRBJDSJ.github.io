# HDFS应用开发
- HDFS在生产应用中主要是客户端的开发,其核心步骤是从HDFS提供的API中构造一个HDFS的访问客户端对象
- 然后通过该客户端对象操作(增删改查)HDFS上的文件

## 搭建开发环境

- 引入依赖
-----------------
	- 注:如需手动引入jar包,HDFS的jar包---hadoop的安装目录的share下

- window下开发的说明
	- 建议在Linux下进行hadoop应用的开发,不会存在兼容性问题.
- 如果window上做客户端应用开发,需要设置以下环境:
	- 在window的某个目录下解压一个hadoop的安装包
	- 将安装包下的lib和bin目录用对应window版本平台编译的本地库替换
	- 在window系统中配置HADOOP_HOME指向你解压的安装包
	- 在Windows系统的path变量中加入hadoop的bin目录

## 获取api中的客户端对象
- 在JAVA红操作HDFS,首先要获取一个客户端实例
	- Configuration conf = new Configuration()
	- FileSystem fs = FileSystem.get(conf)
- 而我们的操作目标是HDFS,所以获取到fs对象应该是DistributedFileSystem的实例

- get方法是从何判断具体实例化那种客户端类呢 !!! ---从conf中的一个参数 fs.defaultFS的配置值判断
- 如果我们的代码中没有指定fs.defaultFS,且工程classpath中也没有给定相应的配置,conf中的默认值就是
- 来自hadoop的jar包的core-default.xml.默认值为: file:/// ,则获取的将不是一个DistributedFileSystem

## DistributedFileSystem 实例对象所具备的方法
----------------------DistributedFileSystem实例方法

## HDFS客户端操作数据代码示例 :

### 文件的增删改查

### 通过流的方式访问HDFS

### 场景编程
- 在mapreduce 、spark等运算框架中，有一个核心思想就是将运算移往数据，或者说，就是要在并发计算中尽可能
- 让运算本地化，这就需要获取数据所在位置的信息并进行相应范围读取
- 以下模拟实现：获取一个文件的所有block位置信息，然后读取指定block中的内容
