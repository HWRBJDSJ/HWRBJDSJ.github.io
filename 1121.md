# azkaban    工作流调度器 

上课跟凯哥的笔记
----------------------------------------------------------------------------------------------

配置  azkaban 需要的文件
azkaban-sql-script-2.5.0.tar.gz  存放azkaban运行需要的sql(sql导完这个文件就没用了)
azkaban-executor-server-2.5.0.tar.gz   存放执行器
azkaban-web-server-2.5.0.tar.gz		 存放web服务器

create-all-sql-2.5.0.sql 在MySQL中执行完整的sql文件，只执行它就够了

ssl 配置 配置之后访问路径为 https://安装了azkaban的服务器的ip:8443

azkaban-executor-2.5.0 启动、关闭执行器 start/shutdown
要配置azkaban.properties	设置时区、MySQL的信息

azkaban-web-2.5.0 启动、关闭web服务器 start/shutdown
要配置azkaban.properties	设置时区、MySQL的信息、jetty密码等同于ssl的秘钥
要配置azkaban-users.xml		设置登录azkaban的用户及其权限

在执行时经常要执行多个job，需要有先后顺序 
这时候用 dependencies=job的名称
来指定当前要执行的job依赖于哪个job ，在依赖的这job执行完再来执行我  这样就产生了先后顺序
除了可以执行我们的Linux命令  像HDFS也可以执行 
还可以执行MapReduce
在执行MapReduce时  job中设置要执行的MapReduce命令，命令中用到的要执行的jar包需要在上传job时
一并压缩，再上传到azkaban 

hive命令也能执行
hive-e;
hive-f '文件名' 这时候不光要上传job本身，还要上传对应的文件，将job和文件一起压缩打包上传
------------------------------------------------------------------------------------------------



例如 ：定时任务

通过command命令来执行
type=command
command=真正要执行的命令



工作流的概念：
# azkaban    工作流调度器 
azkaban 产生的背景：
	一个完整的数据分析系统通常都是由大量任务单元组成：
		shell脚本程序，java程序，mapreduce程序、hive脚本等
	各任务单元之间存在时间先后及前后依赖关系
	为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行；
工作流调度实现方式：
	简单任务调度实现：直接使用Linux的crontab来定义
	复杂任务调度实现：开发调度平台
					  或使用现成的开源调度系统，比如ooize、azkaban等
	
常见工作流调度器：
	四种  ：常见的工作流调度器有Oozie, Azkaban,Cascading,Hamake等
	四种对比：
![四种调度工具的对比.png](https://upload-images.jianshu.io/upload_images/14467497-a6ca07f80c62be07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

工作流的概念：
web容器：
pig:
properties:
（为什么需要配置ssl???）



azkaban与Oozie的对比
功能：
两者均可以调度mapreduce,pig,java,脚本工作流任务
两者均可以定时执行工作流任务

工作流定义：
Azkaban使用Properties文件定义工作流
Oozie使用XML文件定义工作流

工作流传参：
Azkaban支持直接传参，例如${input}
Oozie支持参数和EL表达式，例如${fs:dirSize(myInputDir)}

定时执行：
Azkaban的定时执行任务是基于时间的
Oozie的定时执行任务基于时间和输入数据

资源管理：
Azkaban有较严格的权限控制，如用户对工作流进行读/写/执行等操作
Oozie暂无严格的权限控制

工作流执行：
Azkaban有两种运行模式，分别是solo server mode(executor server和web server部署在同一台节点)和multi server mode(executor server和web server可以部署在不同节点)
Oozie作为工作流服务器运行，支持多用户和多工作流

工作流管理：
Azkaban支持浏览器以及ajax方式操作工作流
Oozie支持命令行、HTTP REST、Java API、浏览器操作工作流




### Azkaban介绍
Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。
它有如下功能特点：
	Web用户界面
	方便上传工作流
	方便设置任务之间的关系
	调度工作流
	认证/授权(权限的工作)
	能够杀死并重新启动工作流
	模块化和可插拔的插件机制
	项目工作区
	工作流和任务的日志记录和审计



azkaban是开源的批量工作流任务调度器 
azkaban安装部署：
	azkaban-web web服务器
	azkaban-executor 执行器
	azkaban-sql	需要借助MySQL 需要向其导入脚本
	
	
	
导入azkaban
选择database ： use + 数据库名
在MySQL执行create-all-sql-2.5.0.sql 导入MySQL脚本
	命令 :source + create-all-sql-2.5.0.sql的路径
![执行createall.png](https://upload-images.jianshu.io/upload_images/14467497-3c455e00233c8498.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在~目录下
创建ssl 配置  

keytool -keystore keystore -alias jetty -genkey -keyalg RSA
![ssl配置1.png](https://upload-images.jianshu.io/upload_images/14467497-4b38d5deff0e4251.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

将生成的keystory 拷贝到  web  根目录下
完成ssl 配置
（为什么需要配置ssl???）
更改azkaban-web服务器配置 
	
	配置文件都在conf文件下！！！！
	
	
	配置 azkaban.properties 
	![azkaban.properties配置1.png](https://upload-images.jianshu.io/upload_images/14467497-60d1ba1d39ea94c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
	服务器properties各级属性：
![1.png](https://upload-images.jianshu.io/upload_images/14467497-364a00412297f7a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![2.png](https://upload-images.jianshu.io/upload_images/14467497-f899f14a04586836.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

	
	配置 user
	添加一条超级用户： <user username="admin" password="admin" roles="admin,metrics" />
	![4.png](https://upload-images.jianshu.io/upload_images/14467497-816d4d7eff48acb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

更改azkaban-executor 执行器配置
	需要更改的文件是azkaban.properties
	更改时区 以及MySQL相关配置 数据库的名字 用户名  密码
	
	执行器的properties的各级属性
	![3.png](https://upload-images.jianshu.io/upload_images/14467497-64b961fe74eaaa72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
	

	
	配置完成 


未配置环境变量
所以执行命令需要到bin目录下

执行 azkaban-executor-start.sh 这个文件

再 执行 bin/azkaban-web-start.sh 文件
启动时产生的错误：
![报错1.png](https://upload-images.jianshu.io/upload_images/14467497-783b692e5db01d7c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
执行权限问题  ：配置路径都是在相对的根目录下的
	
![报错解决.png](https://upload-images.jianshu.io/upload_images/14467497-2ab0d82cd3f964ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![执行jps看端口.png](https://upload-images.jianshu.io/upload_images/14467497-c748dcb0e2805c75.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
图片 jps  端口
进入https://ip:8443    注意要使用https  默认都是HTTP  点高级 选择继续访问  登录admin 密码admin 进入azkaban



进入端口后 新建project 
（让azkaban帮忙执行shell命令）
新建project 

编辑后缀.job的文件  
例1：简单输出语句
![例1.png](https://upload-images.jianshu.io/upload_images/14467497-0480d4a1d4c3f841.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注意 ：将文件压缩为ZIP格式
![上传压缩包出错.png](https://upload-images.jianshu.io/upload_images/14467497-eb3a251bc9fe20e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


压缩ZIP上传成功后 
点execute  进入界面1

![设置执行时间.png](https://upload-images.jianshu.io/upload_images/14467497-47d7744b46930bf7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![设置执行时间细则.png](https://upload-images.jianshu.io/upload_images/14467497-3e408fd1bd257a16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
在此不需要
直接执行点击 execute
跑完变绿色
joblist  ： details（详情）
Flowlog  ： 运行日志

例2：顺序输出语句
![例2.1.png](https://upload-images.jianshu.io/upload_images/14467497-f7d0f9ce4cbdc127.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![例2.2.png](https://upload-images.jianshu.io/upload_images/14467497-2e1c671254e95be7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

有依赖 要一起打包上传  编辑时确定依赖关系dependencies

操作看录屏~！！！ 点点点

注意：azkaban 新建的project 新的ZIP会替换掉旧的ZIP  每一个流程都要新建project

简单命令行后

执行HDFS命令  命令的路径最好使用的是绝对路径
例3：HDFS语句 hadoop简单语句

hadoop已经设置为全局变量可以直接使用  若未设置 需要找到hadoop这个shell命令位置
![例3.png](https://upload-images.jianshu.io/upload_images/14467497-460afd68dfbd1de9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


例4：MapReduce语句Wordcount
![Wordcount.png](https://upload-images.jianshu.io/upload_images/14467497-7d392ee20c985150.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
将jar包一起打包导入


例5.1：hive -e 命令
![hive-e.png](https://upload-images.jianshu.io/upload_images/14467497-37558d77ca6b4cd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
例5.2：hive -f 命令
![例5.png](https://upload-images.jianshu.io/upload_images/14467497-4cee256805502620.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)





# sqoop  数据迁移


迁移方向：
	导入数据
	导出数据

图片

工作机制：


sqoop 安装
解压
配置拷贝sqoop-env-template.sh的sqoop-env.sh
![更改配置sqoop文件1.png](https://upload-images.jianshu.io/upload_images/14467497-104d0caed125b064.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![更改配置sqoop文件.png](https://upload-images.jianshu.io/upload_images/14467497-e84ebf7299d0b413.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



sqoop的数据导入
	将数据导入HDFS
	
![sqoop命令1.png](https://upload-images.jianshu.io/upload_images/14467497-a2523fc3f3116b2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![sqoop命令集.png](https://upload-images.jianshu.io/upload_images/14467497-58f31336a91934b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


基本导入：
![sqoop的import命令.png](https://upload-images.jianshu.io/upload_images/14467497-c96fff318a736ef3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![sqoop的import命令执行结果.png](https://upload-images.jianshu.io/upload_images/14467497-423105d2e051c6ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
从关系型数据库导入数据到HDFS
（因为未指定奥存到HDFS的哪个目录下，所以默认存放在/user/用户名/表名）
并且存储的数据是以“，”号分割的
	bin/sqoop import \
	--connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop \ 
	--username root \
	--password hadoop \
	--table users --m 1

指定输出位置
	bin/sqoop import \
	--connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop \ 
	--username root \
	--password hadoop \
	--target-dir 要存放的路径（不能存在） \ 
	--table users --m 1

指定分割符
bin/sqoop import --connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop --username root --password hadoop --target-dir /sqooptable/userhwr --table users --fields-terminated-by '\t' --m 1

指定输出需求数据
bin/sqoop import --connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop --username root --password hadoop --target-dir /sqooptable/userhwrs --table users --fields-terminated-by '\t' --m 1 --columns name,pwd

将数据导入到hive（未指定存放到哪个库什么表里的时候会默认在default数据库中存放，表名就是关系型数据库中的表名）
bin/sqoop import --connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop --username root --password hadoop --target-dir /sqooptable/hiveusers --table users --m 1 --hive-import 

	查找导过去的文件
	hive 
	show tables；
	select * from users

在hive中往规定的数据库中存储 --hive-table 
bin/sqoop import --connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop --username root --password hadoop --target-dir /sqooptable/hiveusers --table users --m 1 --hive-import --hive-table datahua.hwruser2

若不添加--target-dir /sqooptable/hiveusers  不指定？？ 最后视频35分钟
hive import 作用？？？



导入表的查询结果   --where   还有 --query 

如果想对关系型数据库中的数据进行查询之后再导入到HDFS使用
--query  的语句中有where  结尾必有$CONDITIONS
--query 'sql语句where[id>3] and $CONDITIONS'

如果使用了--query要导入的数据就是我们查询出来的结果集了 就不要设置--table
query错误1
query错误2
正确语句为
bin/sqoop import --connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop --username root --password hadoop --target-dir /sqooptable/hiveusers1  --m 1 --query 'select id,name from users where $CONDITIONS' 




