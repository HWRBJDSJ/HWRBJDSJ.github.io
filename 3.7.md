# sqoop  数据迁移


迁移方向：
	导入数据
	导出数据

图片

工作机制：


sqoop 安装
解压
配置拷贝sqoop-env-template.sh的sqoop-env.sh
![更改配置sqoop文件1.png](https://upload-images.jianshu.io/upload_images/14467497-104d0caed125b064.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![更改配置sqoop文件.png](https://upload-images.jianshu.io/upload_images/14467497-e84ebf7299d0b413.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



sqoop的数据导入
	将数据导入HDFS
	
![sqoop命令1.png](https://upload-images.jianshu.io/upload_images/14467497-a2523fc3f3116b2a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![sqoop命令集.png](https://upload-images.jianshu.io/upload_images/14467497-58f31336a91934b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


基本导入：
![sqoop的import命令.png](https://upload-images.jianshu.io/upload_images/14467497-c96fff318a736ef3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![sqoop的import命令执行结果.png](https://upload-images.jianshu.io/upload_images/14467497-423105d2e051c6ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
从关系型数据库导入数据到HDFS
（因为未指定奥存到HDFS的哪个目录下，所以默认存放在/user/用户名/表名）
并且存储的数据是以“，”号分割的
	bin/sqoop import \
	--connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop \ 
	--username root \
	--password hadoop \
	--table users --m 1

指定输出位置
	bin/sqoop import \
	--connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop \ 
	--username root \
	--password hadoop \
	--target-dir 要存放的路径（不能存在） \ 
	--table users --m 1

指定分割符
bin/sqoop import --connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop --username root --password hadoop --target-dir /sqooptable/userhwr --table users --fields-terminated-by '\t' --m 1

指定输出需求数据
bin/sqoop import --connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop --username root --password hadoop --target-dir /sqooptable/userhwrs --table users --fields-terminated-by '\t' --m 1 --columns name,pwd

将数据导入到hive（未指定存放到哪个库什么表里的时候会默认在default数据库中存放，表名就是关系型数据库中的表名）
bin/sqoop import --connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop --username root --password hadoop --target-dir /sqooptable/hiveusers --table users --m 1 --hive-import 

	查找导过去的文件
	hive 
	show tables；
	select * from users

在hive中往规定的数据库中存储 --hive-table 
bin/sqoop import --connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop --username root --password hadoop --target-dir /sqooptable/hiveusers --table users --m 1 --hive-import --hive-table datahua.hwruser2

若不添加--target-dir /sqooptable/hiveusers  不指定？？ 最后视频35分钟
hive import 作用？？？



导入表的查询结果   --where   还有 --query 

如果想对关系型数据库中的数据进行查询之后再导入到HDFS使用
--query  的语句中有where  结尾必有$CONDITIONS
--query 'sql语句where[id>3] and $CONDITIONS'

如果使用了--query要导入的数据就是我们查询出来的结果集了 就不要设置--table
query错误1
query错误2
正确语句为
bin/sqoop import --connect jdbc:mysql://172.18.24.173:3306/Hwrsqoop --username root --password hadoop --target-dir /sqooptable/hiveusers1  --m 1 --query 'select id,name from users where $CONDITIONS' 



